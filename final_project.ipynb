{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Final Model \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TensorFlow and tf.keras\n",
                "import tensorflow as tf\n",
                "# recommended models\n",
                "from keras.applications import vgg16\n",
                "from keras.applications.vgg19 import VGG19\n",
                "from keras.applications import ResNet50\n",
                "from tensorflow.keras.utils import load_img, img_to_array\n",
                "from keras.applications.imagenet_utils import preprocess_input\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "from tensorflow.keras.models import Sequential, model_from_json\n",
                "from sklearn.model_selection import train_test_split\n",
                "from keras.models import Model\n",
                "from keras.layers import Dense, GlobalAveragePooling2D\n",
                "import keras\n",
                "\n",
                "#CNN Model\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dense, Dropout\n",
                "from tensorflow.keras.optimizers import Adam\n",
                "from tensorflow.keras.callbacks import ModelCheckpoint\n",
                "\n",
                "import datetime\n",
                "# from tensorflow.keras.callbacks import TensorBoard\n",
                "from keras.utils import to_categorical\n",
                "#import resnet50\n",
                "# Helper libraries\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "# get the array with the class names from the folder \n",
                "# read foldernames\n",
                "import os\n",
                "import glob\n",
                "import cv2\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# read all the paths from the folder\n",
                "def get_paths(folder):\n",
                "    paths = []\n",
                "    for root, dirs, files in os.walk(folder):\n",
                "        for file in files:\n",
                "            # if file.endswith(\".jpg\"):\n",
                "            paths.append(os.path.join(root, file))\n",
                "    return paths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "shop_images = get_paths(\n",
                "    r'C:\\Users\\mrpal\\OneDrive\\Desktop\\WBS\\fashcam\\asos')\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## function that loads all models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "from keras.applications.vgg19 import VGG19\n",
                "\n",
                "\n",
                "def get_models():\n",
                "  # load the vgg models\n",
                "  vgg16_model = vgg16.VGG16(weights='imagenet')\n",
                "  vgg19_model = VGG19(weights='imagenet', include_top=False)\n",
                "  # remove the last layers in order to get features instead of predictions\n",
                "  feat_extractor_vgg19 = Model(inputs=vgg19_model.input,\n",
                "                               outputs=vgg19_model.get_layer(\"block5_pool\").output)\n",
                "  feat_extractor = Model(inputs=vgg16_model.input,\n",
                "                         outputs=vgg16_model.get_layer(\"fc2\").output)\n",
                "  # get the model with the last layer in order to get predictions\n",
                "  classifier = Model(inputs=vgg16_model.input,\n",
                "                     outputs=vgg16_model.get_layer(\"predictions\").output)\n",
                "  # print the layers of the CNN\n",
                "  feat_extractor.summary()\n",
                "  # feat_extractor_vgg19.summary()\n",
                "  classifier.summary()\n",
                "  return feat_extractor, classifier, feat_extractor_vgg19\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "feat_extractor = get_models()[0]\n",
                "classifier = get_models()[1]\n",
                "feat_extractor_vgg19 = get_models()[2]\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### function that uses resnet to predict the class and the sparse matrix of image given"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                " ResNet-50 is 50 layers deep and is trained on a million images of 1000 categories from the ImageNet database.\n",
                " the model has over 23 million trainable parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.applications.resnet50 import ResNet50\n",
                "from tensorflow.keras.preprocessing import image\n",
                "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
                "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
                "\n",
                "\n",
                "def get_prediction_resnet(img_path):\n",
                "\n",
                "    # Load the pre-trained ResNet50 model, with the top layer removed\n",
                "    model = InceptionResNetV2(weights='imagenet', include_top=True)\n",
                "\n",
                "    # Load an image to use for prediction\n",
                "    img = load_img(img_path, target_size=(299, 299))\n",
                "    x = img_to_array(img)\n",
                "    x = np.expand_dims(x, axis=0)\n",
                "    x = preprocess_input(x)\n",
                "\n",
                "    # Get the predictions from the model\n",
                "    features = model.predict(x)\n",
                "\n",
                "    # Print the top 5 predictions\n",
                "    predictions = decode_predictions(features, top=2)\n",
                "    # for p in predictions[0]:\n",
                "    #     print(f\"Class: {p[1]}, Probability: {p[2]:.2f}\")\n",
                "    propability = predictions[0][0][2]\n",
                "    return predictions[0][0], propability\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "image_names = os.listdir('asos/only')\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## here we drop outliers or misclasifications and define our unique types"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# read the folder names from the C:\\Users\\mrpal\\OneDrive\\Desktop\\WBS\\fascam\\clothing-dataset-small\\train save them to a list\n",
                "unique_types = [\n",
                "    folder for folder in os.listdir(r'C:\\Users\\mrpal\\OneDrive\\Desktop\\fashcam-v3\\finalDataset')]\n",
                "\n",
                "\n",
                "unique_types\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "SNIPPET TO REMOVE EVERY PHOTO FROM THE FOLDERPATH TO THE OTHER FOLDERS THAT BELONG"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "#import shutil\n",
                "#folderpath= r'C:\\Users\\mrpal\\OneDrive\\Desktop\\WBS\\machineLearning\\supervised\\deeplearning\\Imagesfromfashiondataset\\only' \n",
                "# # for each row in the dataframe\n",
                "# for index, row in selected_styles.iterrows():\n",
                "#     # get the image name\n",
                "#     image_name = row['image_name']\n",
                "#     # get the type of clothing\n",
                "#     type = row['articleType']\n",
                "#     # get the source path to the image\n",
                "#     src = os.path.join(r\"folderpath\", image_name).replace(\"\\\\\", \"/\")\n",
                "  \n",
                "#     # get the destination path to the image\n",
                "#     dst = r'Imagesfromfashiondataset/'+os.path.join(type, image_name).replace(\"\\\\\", \"/\")\n",
                "#     # move the image from the source to the destination\n",
                "#     # print(src, dst)\n",
                "#     shutil.move(src, dst)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Backpacks 200\n",
                        "Belts 611\n",
                        "Bra 477\n",
                        "Caps-hats 180\n",
                        "CasualShoes 572\n",
                        "Dresses 659\n",
                        "Earrings 337\n",
                        "Handbags 94\n",
                        "Heels 43\n",
                        "Leggings 177\n",
                        "Outwear 299\n",
                        "pijamas 141\n",
                        "Ring 118\n",
                        "Sandals 361\n",
                        "Scarves 119\n",
                        "Shirts 459\n",
                        "Shorts 327\n",
                        "Skirts 152\n",
                        "Sportswear 304\n",
                        "Sunglasses 1073\n",
                        "Sweatshirts 582\n",
                        "Tops 257\n",
                        "Trousers 896\n",
                        "Tshirts 3072\n"
                    ]
                }
            ],
            "source": [
                "# give the number of files in the folder with the given path\n",
                "def get_number_of_files(path):\n",
                "    return len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])\n",
                "for i in range(len(unique_types)):\n",
                "    print(unique_types[i], get_number_of_files(r'C:\\Users\\mrpal\\OneDrive\\Desktop\\fashcam-v3\\finalDataset' + '/' + unique_types[i]))    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found 11510 files belonging to 24 classes.\n",
                        "Using 6906 files for training.\n",
                        "Using 4604 files for validation.\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "train_ds, test_ds = tf.keras.utils.image_dataset_from_directory(\n",
                "    r\"C:\\Users\\mrpal\\OneDrive\\Desktop\\fashcam-v3\\finalDataset\",\n",
                "    labels=\"inferred\",\n",
                "    validation_split=0.4,\n",
                "    shuffle=True,\n",
                "    subset=\"both\",\n",
                "    seed=1337,\n",
                "    batch_size=32,\n",
                "    image_size=(224, 224),\n",
                "    label_mode='categorical',\n",
                "    color_mode='rgb',\n",
                "    \n",
                "            )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {},
            "outputs": [],
            "source": [
                "# data = tf.keras.utils.image_dataset_from_directory(\n",
                "\n",
                "#     r\"C:\\Users\\mrpal\\OneDrive\\Desktop\\fashcam-v3\\finalDataset\",\n",
                "#     labels=\"inferred\",\n",
                "#     label_mode='categorical',\n",
                "#     class_names=None,\n",
                "#     color_mode=\"rgb\",\n",
                "#     batch_size=32,\n",
                "#     image_size=(224, 224),\n",
                "#     shuffle=True,\n",
                "#     seed=1337,\n",
                "#     validation_split=0.4,\n",
                "#     subset=\"both\",\n",
                "#     interpolation=\"bilinear\",\n",
                "#     follow_links=False,\n",
                "#     crop_to_aspect_ratio=False,\n",
                "# )\n",
                "# split the data into train and test\n",
                "# train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
                "#     r\"C:\\Users\\mrpal\\OneDrive\\Desktop\\fashcam-v3\\finalDataset\",\n",
                "#     validation_split=0.2,\n",
                "#     subset=\"training\",\n",
                "#     seed=1337,\n",
                "#     label_mode='categorical',\n",
                "#     class_names=None,\n",
                "#     color_mode=\"rgb\",\n",
                "#     batch_size=32,\n",
                "#     image_size=(224, 224),\n",
                "#     image_size=image_size,\n",
                "# )\n",
                "# val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
                "#     r\"C:\\Users\\mrpal\\OneDrive\\Desktop\\fashcam-v3\\finalDataset\",\n",
                "#     validation_split=0.2,\n",
                "#     subset=\"validation\",\n",
                "#     seed=1337,\n",
                "#     label_mode='categorical',\n",
                "#     class_names=None,\n",
                "#     color_mode=\"rgb\",\n",
                "#     batch_size=32,\n",
                "#     image_size=(224, 224),\n",
                "# )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "# import output \n",
                "def train_model_tuning():\n",
                "    # Load the pre-trained ResNet50 model, with the top layer removed\n",
                "    resnet50_model = InceptionResNetV2(\n",
                "        weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
                "    #model with 1 layer\n",
                "    cnn_model = resnet50_model.output\n",
                "    cnn_model = Flatten()(cnn_model)\n",
                "    cnn_model = Dense(len(unique_types), activation='softmax')(cnn_model)\n",
                "    # cnn_model = Dense(len(unique_types), activation='softmax')(cnn_model)\n",
                "    final_model = Model(inputs=resnet50_model.input, outputs=cnn_model)\n",
                "\n",
                "    # Freeze all layers in the base model (except for the top layer)\n",
                "    for layer in resnet50_model.layers:\n",
                "        layer.trainable = False\n",
                "    # Compile the model\n",
                "    final_model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
                "              metrics=['accuracy'])\n",
                "\n",
                "    # Convert the target labels into one-hot encoded format\n",
                "    checkpoint_filepath = r'C:\\Users\\mrpal\\OneDrive\\Desktop\\fashcam-v3'\n",
                "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
                "    filepath=checkpoint_filepath,\n",
                "    save_weights_only=False,\n",
                "    monitor='val_accuracy',\n",
                "    mode='max',\n",
                "    save_best_only=True)\n",
                "    \n",
                "    # Train the model\n",
                "    \n",
                "    history = final_model.fit(\n",
                "        train_ds,\n",
                "        batch_size=100, \n",
                "        epochs=1, \n",
                "        verbose=1,\n",
                "        callbacks=[model_checkpoint_callback],\n",
                "        )\n",
                "  \n",
                "    # save the model\n",
                "    final_model.save(\"final_model_v3.h5\")\n",
                "    # serialize model to JSON\n",
                "    # model_json = final_model.to_json()\n",
                "    # with open(\"final_model.json\", \"w\") as json_file:\n",
                "    #     json_file.write(model_json)\n",
                "    # # serialize weights to HDF5\n",
                "    # final_model.save_weights(\"final_model.h5\")\n",
                "    print(\"Saved model to disk\")\n",
                "    \n",
                "    return 0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# import load model\n",
                "from keras.models import load_model\n",
                "\n",
                "loaded_model = load_model('final_model_v3.h5')\n",
                "\n",
                "# evaluate loaded model on test data\n",
                "loaded_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])       \n",
                "score = loaded_model.evaluate(test_ds[:100], verbose=0)\n",
                "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100)) "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "ename": "TypeError",
                    "evalue": "'BatchDataset' object is not subscriptable",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn [31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m input_data \u001b[39m=\u001b[39m test_ds[:\u001b[39m10\u001b[39;49m]\n\u001b[0;32m      2\u001b[0m input_data \u001b[39m=\u001b[39m input_data\u001b[39m.\u001b[39mas_numpy_iterator()\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m load_model\n",
                        "\u001b[1;31mTypeError\u001b[0m: 'BatchDataset' object is not subscriptable"
                    ]
                }
            ],
            "source": [
                "\n",
                "\n",
                "from keras.models import load_model\n",
                "loaded_model = load_model('final_model_v3.h5')\n",
                "loaded_model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
                "loaded_model.predict(input_data)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "# confusion matrix\n",
                "from sklearn.metrics import confusion_matrix\n",
                "import seaborn as sns   \n",
                "import matplotlib.pyplot as plt \n",
                "loaded_model.compile(\n",
                "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
                "\n",
                "y_pred = loaded_model.predict(test_ds)\n",
                "y_pred = np.argmax(y_pred, axis=1)\n",
                "y_true = test_ds.classes\n",
                "cm = confusion_matrix(y_true, y_pred)\n",
                "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
                "plt.figure(figsize=(10,10))\n",
                "sns.heatmap(cm, annot=True, fmt='.2f', xticklabels=unique_types, yticklabels=unique_types)\n",
                "plt.ylabel('True label')\n",
                "plt.xlabel('Predicted label')\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### function that uses fine tunin with resnet50 to predict the class and the sparse matrix of image given"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_prediction_tuning(img_path):\n",
                "    # load json and create model\n",
                "    json_file = open('final_model.json', 'r')\n",
                "    loaded_model_json = json_file.read()\n",
                "    json_file.close()\n",
                "    loaded_model = model_from_json(loaded_model_json)\n",
                "    # load weights into new model\n",
                "    loaded_model.load_weights(\"final_model.h5\")\n",
                "    # Load an image to use for prediction\n",
                "    img = load_img(img_path, target_size=(224, 224,3))\n",
                "    x = img_to_array(img)\n",
                "    x = np.expand_dims(x, axis=0)\n",
                "    x = preprocess_input(x)\n",
                "    # result = loaded_model.predict(x)\n",
                "    # evaluate loaded model on test data\n",
                "    loaded_model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
                "                         metrics=['accuracy'])\n",
                "    result = loaded_model.predict(x)\n",
                "    # get the percentage of the prediction\n",
                "    \n",
                "    \n",
                "    return (result,unique_types[np.argmax(result)])\n",
                "    \n",
                "    \n",
                "    \n",
                "  \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'test_x' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn [21], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m Y_pred \u001b[39m=\u001b[39m loaded_model\u001b[39m.\u001b[39mpredict(test_x[:\u001b[39m50\u001b[39m])\n\u001b[0;32m      7\u001b[0m y_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(test_y, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m y_true \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(test_y[:\u001b[39m50\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
                        "\u001b[1;31mNameError\u001b[0m: name 'test_x' is not defined"
                    ]
                }
            ],
            "source": [
                "# # confusion matrix\n",
                "#import seaborn as sns\n",
                "from sklearn.metrics import confusion_matrix\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "Y_pred = loaded_model.predict(test_x[:50])\n",
                "y_pred = np.argmax(test_y, axis=1)\n",
                "y_true = np.argmax(test_y[:50], axis=1)\n",
                "confusion_matrix(y_true, y_pred)\n",
                "cm = confusion_matrix(y_true, y_pred)   \n",
                "plt.figure(figsize=(20,20))\n",
                "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
                "plt.title(\"Confusion matrix\")\n",
                "plt.ylabel('True label')\n",
                "plt.xlabel('Predicted label')\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### function that uses vgg16 model to predict the class and the sparse matrix of image given"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "VGG-19 is an improved version of vgg16 and it is 19 layers deep.  a pretrained version of the network trained on more than a million images from the ImageNet database. The pretrained network can classify images into 1000 object categories, such as keyboard, mouse, pencil, and many animals"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_prediction_vgg16(img_path):\n",
                "    # imgs_model_width, imgs_model_height = 224, 224\n",
                "    # load the model\n",
                "    vgg_model = vgg16.VGG16(weights='imagenet')\n",
                "\n",
                "    original = load_img(img_path, target_size=(224, 224))\n",
                "    numpy_image = img_to_array(original)\n",
                "    image_batch = np.expand_dims(numpy_image, axis=0)\n",
                "    images = np.vstack([image_batch])\n",
                "\n",
                "    processed_img = preprocess_input(images.copy())\n",
                "    prediction = classifier.predict(images)\n",
                "    result = feat_extractor.predict(images)\n",
                "    # help the model to predict the class\n",
                "    return result, vgg16.decode_predictions(prediction, top=1)[0][0][1]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # load the model\n",
                "# vgg_model = vgg16.VGG16(weights='imagenet')\n",
                "\n",
                "# # remove the last layers in order to get features instead of predictions\n",
                "\n",
                "# feat_extractor = Model(inputs=vgg_model.input,\n",
                "#                        outputs=vgg_model.get_layer(\"fc2\").output)\n",
                "# # get the model with the last layer in order to get predictions\n",
                "# classifier = Model(inputs=vgg_model.input,\n",
                "#                      outputs=vgg_model.get_layer(\"predictions\").output)\n",
                "\n",
                "# # print the layers of the CNN\n",
                "# feat_extractor.summary()\n",
                "# classifier.summary()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### testing all models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1/1 [==============================] - 0s 82ms/step\n"
                    ]
                },
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mCanceled future for execute_request message before replies were done"
                    ]
                }
            ],
            "source": [
                "test_file = r\"C:\\Users\\mrpal\\OneDrive\\Desktop\\WBS\\machineLearning\\supervised\\deeplearning\\image3.jpg\"\n",
                "\n",
                "get_prediction_vgg16(test_file)[1]\n",
                "get_prediction_resnet(test_file)[1]\n",
                "get_prediction_tuning(test_file)[1]\n",
                "# give a black frame to the picture through load image and then show it:\n",
                "original = load_img(test_file)\n",
                "plt.imshow(original)\n",
                "print(\"vgg16 says \" + get_prediction_vgg16(test_file)[1])\n",
                "print(\"resnet says \" + get_prediction_resnet(test_file)[1])\n",
                "print (\"tuned model says \" + get_prediction_tuning(test_file)[1])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# feat_extractor.predict(preprocess_input(np.expand_dims(load_img(r'C:\\Users\\mrpal\\OneDrive\\Desktop\\WBS\\machineLearning\\supervised\\deeplearning\\FasCam\\clothing-dataset-small\\dress\\009b3c31-fb62-45c0-be9a-37a5c238cb88.jpg', target_size=(imgs_model_width, imgs_model_height)), axis=0)))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### function that gets a list of images and returns the features of those images"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# function that gets a list of images and returns the features of those images\n",
                "def get_features(img_paths, category_folder_name):\n",
                "    imgs_model_width, imgs_model_height = 224, 224\n",
                "    nb_closest_images = 5\n",
                "    importedImages = []\n",
                "    for f in img_paths:\n",
                "        filename = f\n",
                "        original = load_img(filename, target_size=(224, 224))\n",
                "        numpy_image = img_to_array(original)\n",
                "        image_batch = np.expand_dims(numpy_image, axis=0)\n",
                "\n",
                "        importedImages.append(image_batch)\n",
                "    images = np.vstack(importedImages)\n",
                "    processed_imgs = preprocess_input(images.copy())\n",
                "    # load the model\n",
                "    vgg_model = vgg16.VGG16(weights='imagenet')\n",
                "    # remove the last layers in order to get features instead of predictions\n",
                "    feat_extractor = Model(inputs=vgg_model.input,\n",
                "                           outputs=vgg_model.get_layer(\"fc2\").output)\n",
                "    # print the layers of the CNN\n",
                "    feat_extractor.summary()\n",
                "    imgs_features = feat_extractor.predict(processed_imgs)\n",
                "    print(\"features successfully extracted!\")\n",
                "    # save it as csv file\n",
                "    np.savetxt(f'{category_folder_name}.csv', imgs_features, delimiter=\",\")\n",
                "    return imgs_features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "get_features(list_directory[:5000], 'imagesfromfashiondata')\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### function that takes an image as input and applies the cosine similarity with the features from csv to get the closest images\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# function that takes an image as input and applies the cosine similarity with the features from csv to get the closest images\n",
                "def get_closest_images(img_path, nb_closest_images=4):\n",
                "    # prediction about the class of the image\n",
                "    available_class = get_prediction_tuning(img_path)\n",
                "    available_class = \"Outwear\"\n",
                "    # load the features from csv\n",
                "\n",
                "    features = np.genfromtxt('Outwear.csv', delimiter=',')\n",
                "    # get the features of the image\n",
                "\n",
                "    model = ResNet50(weights='imagenet', include_top=True)\n",
                "    feat_extractor = Model(\n",
                "        inputs=model.input, outputs=model.get_layer(\"avg_pool\").output)\n",
                "    feat_extractor.summary()\n",
                "\n",
                "    img_features = feat_extractor.predict(preprocess_input(\n",
                "        np.expand_dims(load_img(img_path, target_size=(224, 224)), axis=0)))\n",
                "    # get the cosine similarity\n",
                "    cosSimilarities = cosine_similarity(img_features, features)\n",
                "\n",
                "    print(\"-----------------------------------------------------------------------\")\n",
                "    print(\"original product:\")\n",
                "    original = load_img(img_path, target_size=(\n",
                "        224, 224))\n",
                "    plt.imshow(original)\n",
                "    plt.show()\n",
                "    print(\"-----------------------------------------------------------------------\")\n",
                "    print(\"most similar products:\")\n",
                "    # get the indexes of the closest images\n",
                "    closest_imgs_indexes = cosSimilarities.argsort()[0][-nb_closest_images:]\n",
                "    # similarity score of the closest images\n",
                "    closest_imgs_similarities = cosSimilarities[0][closest_imgs_indexes]\n",
                "\n",
                "    # get the closest images\n",
                "    closest_imgs = [shopfiles[i]\n",
                "                    for i in closest_imgs_indexes if i < len(shopfiles)]\n",
                "    for i in range(0, nb_closest_images):\n",
                "\n",
                "        original = load_img(closest_imgs[i], target_size=(\n",
                "            224, 224))\n",
                "        plt.imshow(original)\n",
                "        plt.show()\n",
                "        print(\"similarity score:\", closest_imgs_similarities[i])\n",
                "    return 0\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.4"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "e5d44d20471fed6b31c84e96a507e39677b7979bf00486c2e6552218c91082f0"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
